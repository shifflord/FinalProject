{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mask(n_context: int) -> Float[torch.Tensor, \"n_context n_context\"]:\n",
    "def create_mask(n_context: int) -> torch.Tensor:\n",
    "    mask = torch.zeros(n_context, n_context)\n",
    "    indices = torch.triu_indices(n_context, n_context, offset=1)\n",
    "    mask[indices[0], indices[1]] = float('-inf') \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration class for our transformer\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\td_vocab: int = 10_000 \n",
    "\td_model: int = 128\n",
    "\td_mlp: int = 512\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 32\n",
    "\tn_layers: int = 6\n",
    "\tmax_ctx: int = 512\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# Define learned attention matrices\n",
    "\t\tself.W_Q = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\t\tself.W_K = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\t\tself.W_O = nn.Linear(cfg.d_head, cfg.d_model)\n",
    "\t\tself.W_V = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\tM = create_mask(x.size(0)).to(x.device) # Make sure mask is on same device as input tensor\n",
    "\t\treturn F.softmax(self.W_Q(x) @ self.W_K(x).T + M) @ self.W_O(self.W_V(x)) # Attention equation\n",
    "\t\t\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# List of attention heads\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(cfg) for i in range(cfg.n_heads)])\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\thead_outputs = [head(x) for head in self.heads]  # List of tensors\n",
    "\t\tsum_output = sum(head_outputs) # Adds all head outputs together\n",
    "\n",
    "\t\treturn sum_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# Define layers of MLP\n",
    "\t\tself.Hidden = nn.Linear(cfg.d_model, cfg.d_mlp)\n",
    "\t\tself.Output = nn.Linear(cfg.d_mlp, cfg.d_model)\n",
    "\n",
    "\t\t# Using GELU activation function\n",
    "\t\tself.gelu = nn.GELU()\n",
    "\t\t\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\t\t# It's an MLP\n",
    "\t\treturn self.gelu(self.Output(self.gelu(self.Hidden(x))))\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# Creates positional embedding matrix that covers all possible context lengths\n",
    "\t\tself.pos_embedding = nn.Embedding(cfg.max_ctx, cfg.d_model)\n",
    "\n",
    "\t\t# Embedding and unembedding matrices\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\t\tself.unembedding = nn.Linear(cfg.d_model, cfg.d_vocab)\t\n",
    "\n",
    "\t\t# Layernorm\n",
    "\t\tself.norm = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "\t\t# Creates dictionary of attention heads and mlps depending on transformer depth\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\tnn.ModuleDict({\n",
    "\t\t\t\t'attn': MultiHeadedAttention(cfg),\n",
    "\t\t\t\t'mlp': MLP(cfg)\n",
    "\t\t\t}) for _ in range(cfg.n_layers)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "  \n",
    "\t\tx = self.embedding(x) # shape (n_context, d_model)\n",
    "\t\t\n",
    "\t\tpositions = torch.arange(x.size(0), device=x.device)  # (n_context, 1)\n",
    "\t\tpos_emb = self.pos_embedding(positions)  # (n_context, d_model)\n",
    "\n",
    "\t\tx = x + pos_emb\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\t\n",
    "\t\t\t# Residual connections for each layer\n",
    "\t\t\tx = x + layer['attn'](self.norm(x)) \n",
    "\t\t\tx = x + layer['mlp'](self.norm(x))\n",
    "\t\t\n",
    "\t\treturn self.unembedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 15...\n",
      "\t1218778 characters read\n",
      "Getting book 63...\n",
      "\t103137 characters read\n",
      "Getting book 16...\n",
      "\t255644 characters read\n",
      "Getting book 41...\n",
      "\t69837 characters read\n",
      "Getting book 14...\n",
      "\t1897512 characters read\n",
      "Getting book 76...\n",
      "\t571137 characters read\n",
      "Getting book 99...\n",
      "\t45748 characters read\n",
      "Getting book 209...\n",
      "\t228531 characters read\n",
      "Getting book 110...\n",
      "\t841270 characters read\n",
      "Getting book 9...\n",
      "\t21311 characters read\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from get_books import get_many_books\n",
    "\n",
    "# Import some books and pull them all into one giant string\n",
    "book_ids = [15, 63, 16, 41, 14, 76, 99, 209, 110, 9]\n",
    "dataset = get_many_books(book_ids, data_temp=\"./data/gutenberg_data\")\n",
    "rawtext = \"\"\n",
    "for book in dataset:\n",
    "    rawtext += book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook: {'b': '1111010', 'c': '101110', 'f': '011010', 'v': '0100110', ')': '101111100', 'w': '011001', 'k': '0001100', 'P': '101000001', '*': '11110110000010', '4': '01000101', '“': '010011100', '-': '0110000', ']': '111101100000110', 'T': '101111110', '£': '101000000010110111010', ';': '10100011', 'U': '1010100101', 'ח': '1010000000101100101010', 'q': '11110110110', '!': '0001101000', 'N': '010011101', '.': '0100011', 'æ': '10100000001011000', 'i': '0010', '0': '11110111', 'C': '101111000', 'ο': '1010000000101100100011', 'D': '1111011010', 'E': '000111101', '1': '0001110', 'ù': '1010000000101101111110', 'ו': '1010000000101100101001', ':': '01100011', 'Y': '10100000000', 'η': '1010000000101100101011', 's': '0000', 'h': '10110', '—': '1010000001', '$': '10101001110', '/': '101010011110', 'Z': '1010000000100', 't': '1001', '[': '101000000010111', 'o': '0101', 'V': '101010011111', 'G': '1010100110', '6': '00011111', '_': '10101011111', 'L': '1011111010', 'Q': '1010000000110', '’': '101010100', 'W': '1011111111', 'Æ': '1010000000101100100100', 'à': '101000000010110010000', 'a': '0111', '8': '10100010', '′': '10100000001011011100', ' ': '110', 'M': '000110101', 'Œ': '10100000001011011111111', 'A': '00011011', 'e': '1110', 'ï': '10100000001011011111110', '\\n': '00010', 'τ': '1010000000101100101000', \"'\": '1010000000111', 'X': '1111011000000', 'x': '1011111110', '{': '10100000001011011011', 'F': '1010101010', 'H': '1010101011', 'l': '10001', '&': '10100000001011011110', 'r': '11111', '2': '10100001', 'J': '11110110001', 'ç': '1010000000101100100111', 'â': '1010000000101100100110', 'j': '11110110111', 'S': '101010110', 'œ': '10100000001011011010', 'u': '111100', '?': '10101011110', '7': '01000100', '}': '10100000001011001011', 'm': '101011', 'n': '0011', 'd': '10000', 'é': '101000000010110011', 'è': '101000000010110111011', '‘': '111101100000111', 'I': '10101000', 'R': '1010100100', 'y': '010010', '%': '1111011001', '5': '01100010', 'ς': '1010000000101100100101', '”': '000111100', '3': '01001111', '=': '10100000001010', ',': '101001', 'z': '0001101001', '9': '10111101', 'B': '1010101110', '\"': '1010000000101101100', 'K': '111101100001', 'p': '010000', 'ê': '101000000010110111110', 'ϰ': '1010000000101100100010', 'O': '1011111011', '+': '10100000001011010', 'g': '011011', '(': '101111001'}\n",
      "Encoded data: 00010000110101010111110100100100110000111101101000\n",
      "Binary file with encoded data has been written as 'encoded_data.bin'.\n"
     ]
    }
   ],
   "source": [
    "import huffman\n",
    "\n",
    "codebook = huffman.codebook((char, rawtext.count(char)) for char in set(rawtext))\n",
    "\n",
    "print(\"Codebook:\", codebook)\n",
    "\n",
    "# Encoding\n",
    "encoded = ''.join(codebook[char] for char in rawtext)\n",
    "print(\"Encoded data:\", encoded[0:50])\n",
    "\n",
    "padding_length = 8 - len(encoded) % 8\n",
    "encoded_padded = encoded + '0' * padding_length  # Add padding (if necessary)\n",
    "\n",
    "# Convert the padded binary string into a byte array\n",
    "byte_array = bytearray()\n",
    "for i in range(0, len(encoded_padded), 8):\n",
    "    byte_array.append(int(encoded_padded[i:i+8], 2))  # Convert each 8-bit chunk to a byte\n",
    "\n",
    "# Write the binary data to a file\n",
    "with open(\"encoded_data.bin\", \"wb\") as f:\n",
    "    # Write the padding length at the beginning (so we can decode it later)\n",
    "    f.write(bytes([padding_length]))\n",
    "    f.write(byte_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSI-like string: \u0001\u0010Õ}$ÃÚ+KúD+õV=×ýV7}\u000fQ^Õ_~·<5è¦(Ç!^\u0014ç@BÃÙJµ¶°öþ½Ò\u0002\u0016¼Uct\u001bðö¤è\u001béV²6ÀZñUÐoÃÚTT# oÖíx¡Óªçl\n"
     ]
    }
   ],
   "source": [
    "with open(\"encoded_data.bin\", \"rb\") as f:\n",
    "    byte_data = f.read()\n",
    "\n",
    "# Decode using latin-1, a safe 1-to-1 mapping\n",
    "ansi_like_string = byte_data.decode('latin-1')\n",
    "\n",
    "print(\"ANSI-like string:\", ansi_like_string[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Tokenize the entire dataset (ignore warning about sequence length thrown here)\n",
    "# tokens = tokenizer(rawtext, return_tensors=\"pt\")\n",
    "\n",
    "# # Reorganize tokens into lengths of chunk_size\n",
    "# chunk_size = 100\n",
    "# to_remove = tokens[\"input_ids\"].shape[1] % chunk_size\n",
    "# new_shape = tokens[\"input_ids\"].shape[1] // chunk_size\n",
    "# attention_mask = tokens['attention_mask'][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "# input_ids = tokens['input_ids'][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "\n",
    "# # Format tokens for dataloader and load them in\n",
    "# tensor = TensorDataset(input_ids, attention_mask)\n",
    "# dataloader = DataLoader(tensor, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = Transformer(model_cfg).to(device)\n",
    "\n",
    "# # Set up optimizer and loss\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # Put model into training mode to store derivatives (important to do this after it is on gpu)\n",
    "# model.train()\n",
    "\n",
    "\n",
    "# n_epochs = 1\n",
    "# print_interval = 10\n",
    "# for epoch in range(n_epochs):\n",
    "#     for step, batch in enumerate(dataloader):\n",
    "#         # [batch_size, seq_len]\n",
    "#         input_ids_batch = batch[0].to(device)\n",
    "\n",
    "#         # We'll accumulate the losses for each sequence in this mini-batch\n",
    "#         total_loss = 0.0\n",
    "#         batch_size = input_ids_batch.size(0)\n",
    "        \n",
    "#         # Process each sequence individually\n",
    "#         for i in range(batch_size):\n",
    "#             # Extract a single sequence of shape [seq_len]\n",
    "#             seq_ids = input_ids_batch[i]\n",
    "\n",
    "#             # Next-token language modeling: input is all but last token, target is all but first\n",
    "#             inp = seq_ids[:-1]   # shape [seq_len - 1]\n",
    "#             targ = seq_ids[1:]    # shape [seq_len - 1]\n",
    "\n",
    "#             # Forward pass\n",
    "#             # Your model returns logits of shape [seq_len-1, d_vocab]\n",
    "#             logits = model(inp)\n",
    "\n",
    "#             # Compute loss across this sequence\n",
    "#             # CrossEntropyLoss expects [batch, vocab], so we can pass [seq_len-1, d_vocab] vs. [seq_len-1]\n",
    "#             loss = criterion(logits, targ)\n",
    "\n",
    "#             # Accumulate\n",
    "#             total_loss += loss\n",
    "\n",
    "#         # Average across all sequences in the batch\n",
    "#         total_loss = total_loss / batch_size\n",
    "\n",
    "#         # Backprop and update\n",
    "#         optimizer.zero_grad()\n",
    "#         total_loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Print progress\n",
    "#         if (step + 1) % print_interval == 0:\n",
    "#             print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "# # The loss initially drops fast, and then it becomes more gradual over time. \n",
    "# # Due to the nature of sgdm, the loss doesn't always decrease after every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Move the model back to the cpu for inference\n",
    "# model.to('cpu')\n",
    "# model.eval()\n",
    "\n",
    "# # Generate function\n",
    "# def generate(input_text: str, output_tokens: int, model) -> str:\n",
    "#     for new_token in range(output_tokens):\n",
    "        \n",
    "#         new_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "#         with torch.no_grad():\n",
    "#             out_probs = F.softmax(model(new_tokens['input_ids'][0]), dim=-1) \n",
    "#         samples = torch.multinomial(out_probs, 1)\n",
    "#         detokenized_text = tokenizer.decode(samples[-1][0], skip_special_tokens=True)\n",
    "#         print(detokenized_text,end='')\n",
    "#         input_text += detokenized_text\n",
    "    \n",
    "#     return input_text\n",
    "\n",
    "# # Its alive... sorta\n",
    "# test_output = generate('Why are there so many new lines?', 50, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
