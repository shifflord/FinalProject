{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mask(n_context: int) -> Float[torch.Tensor, \"n_context n_context\"]:\n",
    "def create_mask(n_context: int) -> torch.Tensor:\n",
    "    mask = torch.zeros(n_context, n_context)\n",
    "    indices = torch.triu_indices(n_context, n_context, offset=1)\n",
    "    mask[indices[0], indices[1]] = float('-inf') \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration class for our transformer\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\td_vocab: int = 10_000 \n",
    "\td_model: int = 256\n",
    "\td_mlp: int = 1024\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 64\n",
    "\tn_layers: int = 6\n",
    "\tmax_ctx: int = 512\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# Define learned attention matrices\n",
    "\t\tself.W_Q = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\t\tself.W_K = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\t\tself.W_O = nn.Linear(cfg.d_head, cfg.d_model)\n",
    "\t\tself.W_V = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\tM = create_mask(x.size(0)).to(x.device) # Make sure mask is on same device as input tensor\n",
    "\t\treturn F.softmax(self.W_Q(x) @ self.W_K(x).T + M) @ self.W_O(self.W_V(x)) # Attention equation\n",
    "\t\t\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# List of attention heads\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(cfg) for i in range(cfg.n_heads)])\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\thead_outputs = [head(x) for head in self.heads]  # List of tensors\n",
    "\t\tsum_output = sum(head_outputs) # Adds all head outputs together\n",
    "\n",
    "\t\treturn sum_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# Define layers of MLP\n",
    "\t\tself.Hidden = nn.Linear(cfg.d_model, cfg.d_mlp)\n",
    "\t\tself.Output = nn.Linear(cfg.d_mlp, cfg.d_model)\n",
    "\n",
    "\t\t# Using GELU activation function\n",
    "\t\tself.gelu = nn.GELU()\n",
    "\t\t\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\t\t# It's an MLP\n",
    "\t\treturn self.gelu(self.Output(self.gelu(self.Hidden(x))))\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# Creates positional embedding matrix that covers all possible context lengths\n",
    "\t\tself.pos_embedding = nn.Embedding(cfg.max_ctx, cfg.d_model)\n",
    "\n",
    "\t\t# Embedding and unembedding matrices\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\t\tself.unembedding = nn.Linear(cfg.d_model, cfg.d_vocab)\t\n",
    "\n",
    "\t\t# Layernorm\n",
    "\t\tself.norm = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "\t\t# Creates dictionary of attention heads and mlps depending on transformer depth\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\tnn.ModuleDict({\n",
    "\t\t\t\t'attn': MultiHeadedAttention(cfg),\n",
    "\t\t\t\t'mlp': MLP(cfg)\n",
    "\t\t\t}) for _ in range(cfg.n_layers)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "  \n",
    "\t\tx = self.embedding(x) # shape (n_context, d_model)\n",
    "\t\t\n",
    "\t\tpositions = torch.arange(x.size(0), device=x.device)  # (n_context, 1)\n",
    "\t\tpos_emb = self.pos_embedding(positions)  # (n_context, d_model)\n",
    "\n",
    "\t\tx = x + pos_emb\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\t\n",
    "\t\t\t# Residual connections for each layer\n",
    "\t\t\tx = x + layer['attn'](self.norm(x)) \n",
    "\t\t\tx = x + layer['mlp'](self.norm(x))\n",
    "\t\t\n",
    "\t\treturn self.unembedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 15...\n",
      "\t1218778 characters read\n",
      "Getting book 63...\n",
      "\t103137 characters read\n",
      "Getting book 16...\n",
      "\t255644 characters read\n",
      "Getting book 41...\n",
      "\t69837 characters read\n",
      "Getting book 14...\n",
      "\t1897512 characters read\n",
      "Getting book 76...\n",
      "\t571137 characters read\n",
      "Getting book 99...\n",
      "\t45748 characters read\n",
      "Getting book 209...\n",
      "\t228531 characters read\n",
      "Getting book 110...\n",
      "\t841270 characters read\n",
      "Getting book 9...\n",
      "\t21311 characters read\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from get_books import get_many_books\n",
    "\n",
    "# Import some books and pull them all into one giant string\n",
    "book_ids = [15, 63, 16, 41, 14, 76, 99, 209, 110, 9]\n",
    "dataset = get_many_books(book_ids, data_temp=\"./data/gutenberg_data\")\n",
    "rawtext = \"\"\n",
    "for book in dataset:\n",
    "    rawtext += book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.vocab_size)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Tokenize the entire dataset (ignore warning about sequence length thrown here)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m tokens = \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Reorganize tokens into lengths of chunk_size\u001b[39;00m\n\u001b[32m     13\u001b[39m chunk_size = \u001b[32m100\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2887\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2885\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2886\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2887\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2889\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2997\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2976\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2977\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2994\u001b[39m         **kwargs,\n\u001b[32m   2995\u001b[39m     )\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3000\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3016\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3073\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3063\u001b[39m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[32m   3064\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3065\u001b[39m     padding=padding,\n\u001b[32m   3066\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3070\u001b[39m     **kwargs,\n\u001b[32m   3071\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3092\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\tokenization_utils.py:800\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    793\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    794\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    797\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    798\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m first_ids = \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    801\u001b[39m second_ids = get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_for_model(\n\u001b[32m    804\u001b[39m     first_ids,\n\u001b[32m    805\u001b[39m     pair_ids=second_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     verbose=verbose,\n\u001b[32m    821\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\tokenization_utils.py:767\u001b[39m, in \u001b[36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    765\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_input_ids\u001b[39m(text):\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m767\u001b[39m         tokens = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convert_tokens_to_ids(tokens)\n\u001b[32m    769\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[32m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\tokenization_utils.py:697\u001b[39m, in \u001b[36mPreTrainedTokenizer.tokenize\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    695\u001b[39m         tokenized_text.append(token)\n\u001b[32m    696\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m         tokenized_text.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py:278\u001b[39m, in \u001b[36mGPT2Tokenizer._tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    276\u001b[39m bpe_tokens = []\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m     token = \u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbyte_encoder\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m    281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe(token).split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\FinalProject\\venv\\Lib\\site-packages\\transformers\\models\\gpt2\\tokenization_gpt2.py:279\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    276\u001b[39m bpe_tokens = []\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re.findall(\u001b[38;5;28mself\u001b[39m.pat, text):\n\u001b[32m    278\u001b[39m     token = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m         \u001b[38;5;28mself\u001b[39m.byte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     )  \u001b[38;5;66;03m# Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\u001b[39;00m\n\u001b[32m    281\u001b[39m     bpe_tokens.extend(bpe_token \u001b[38;5;28;01mfor\u001b[39;00m bpe_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bpe(token).split(\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bpe_tokens\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "\n",
    "# Load pretrained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer.vocab_size)\n",
    "\n",
    "# Tokenize the entire dataset (ignore warning about sequence length thrown here)\n",
    "tokens = tokenizer(rawtext, return_tensors=\"pt\")\n",
    "\n",
    "# Reorganize tokens into lengths of chunk_size\n",
    "chunk_size = 100\n",
    "to_remove = tokens[\"input_ids\"].shape[1] % chunk_size\n",
    "new_shape = tokens[\"input_ids\"].shape[1] // chunk_size\n",
    "attention_mask = tokens['attention_mask'][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "input_ids = tokens['input_ids'][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "\n",
    "# Format tokens for dataloader and load them in\n",
    "tensor = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(tensor, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Torch built with CUDA: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Torch built with CUDA:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgo\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clear_output\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model_cfg = GPTConfig(d_vocab=tokenizer.vocab_size) # Configure the model for GPT2 vocab size\n",
    "model = Transformer(model_cfg).to(device)\n",
    "\n",
    "# Set up optimizer and loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Put model into training mode to store derivatives (important to do this after it is on gpu)\n",
    "model.train()\n",
    "\n",
    "# Preâ€define layout so each redraw looks identical\n",
    "layout = go.Layout(\n",
    "    title=\"Training Loss Over Time\",\n",
    "    xaxis=dict(title=\"Step\"),\n",
    "    yaxis=dict(title=\"Loss\"),\n",
    ")\n",
    "\n",
    "losses = []\n",
    "\n",
    "n_epochs = 1\n",
    "print_interval = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # [batch_size, seq_len]\n",
    "        input_ids_batch = batch[0].to(device)\n",
    "\n",
    "        # We'll accumulate the losses for each sequence in this mini-batch\n",
    "        total_loss = 0.0\n",
    "        batch_size = input_ids_batch.size(0)\n",
    "        \n",
    "        # Process each sequence individually\n",
    "        for i in range(batch_size):\n",
    "            # Extract a single sequence of shape [seq_len]\n",
    "            seq_ids = input_ids_batch[i]\n",
    "\n",
    "            # Next-token language modeling: input is all but last token, target is all but first\n",
    "            inp = seq_ids[:-1]   # shape [seq_len - 1]\n",
    "            targ = seq_ids[1:]    # shape [seq_len - 1]\n",
    "\n",
    "            # Forward pass\n",
    "            # Your model returns logits of shape [seq_len-1, d_vocab]\n",
    "            logits = model(inp)\n",
    "\n",
    "            # Compute loss across this sequence\n",
    "            # CrossEntropyLoss expects [batch, vocab], so we can pass [seq_len-1, d_vocab] vs. [seq_len-1]\n",
    "            loss = criterion(logits, targ)\n",
    "\n",
    "            # Accumulate\n",
    "            total_loss += loss\n",
    "\n",
    "        # Average across all sequences in the batch\n",
    "        total_loss = total_loss / batch_size\n",
    "\n",
    "        # Backprop and update\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record and push update to Plotly trace\n",
    "        losses.append(total_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        if (step + 1) % print_interval == 0:\n",
    "            # redraw the figure\n",
    "            clear_output(wait=True)\n",
    "            fig = go.Figure(\n",
    "                data=[go.Scatter(x=list(range(len(losses))),\n",
    "                                y=losses,\n",
    "                                mode=\"lines\",\n",
    "                                name=\"Training Loss\")],\n",
    "                layout=layout\n",
    "            )\n",
    "            fig.show(renderer=\"notebook\")  # or omit renderer in many cases\n",
    "            print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "# The loss initially drops fast, and then it becomes more gradual over time. \n",
    "# Due to the nature of sgdm, the loss doesn't always decrease after every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Move the model back to the cpu for inference\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate function\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Move the model back to the cpu for inference\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "# Generate function\n",
    "def generate(input_text: str, output_tokens: int, model) -> str:\n",
    "    for new_token in range(output_tokens):\n",
    "        \n",
    "        new_tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            out_probs = F.softmax(model(new_tokens['input_ids'][0]), dim=-1) \n",
    "        samples = torch.multinomial(out_probs, 1)\n",
    "        detokenized_text = tokenizer.decode(samples[-1][0], skip_special_tokens=True)\n",
    "        print(detokenized_text,end='')\n",
    "        input_text += detokenized_text\n",
    "    \n",
    "    return input_text\n",
    "\n",
    "# Its alive... sorta\n",
    "test_output = generate('Give me a home where the buffalo roam', 100, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
